{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f042f017-1827-4760-8314-0aec855fbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lz4\n",
    "import nibabel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.nn import pyro_method, DenseNN\n",
    "from pyro.distributions.conditional import ConditionalTransformModule\n",
    "import sys\n",
    "sys.path.insert(0,\"../data/\")\n",
    "import torch\n",
    "from loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "109db419-2233-4102-ac35-1d90ce9e8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if (type(m) == nn.Conv3d or\n",
    "        type(m) == nn.ConvTranspose2d or\n",
    "        type(m) == nn.Conv2d\n",
    "       or type(m)== nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight,mode='fan_in',a=.01,nonlinearity=\"leaky_relu\")\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41781f34-547c-42a9-9eb6-0581541270c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(x,x1,x2,y1,y2,z1,z2):\n",
    "    return x[x1:x2,y1:y2,z1:z2]\n",
    "def downsample(x,factorx,factory=None,factorz=None):\n",
    "    if factory==None:\n",
    "        factory=factorx\n",
    "    if factorz==None:\n",
    "        factorz=factorx\n",
    "    return x[::factorx,::factory,::factorz]\n",
    "def brain_slice(x,slice_num):\n",
    "    return x[:,:,slice_num]\n",
    "def normalize(x,norm_type=None):\n",
    "    x-=np.min(x,keepdims=True)\n",
    "    x/=(np.max(x,keepdims=True)+.00001)\n",
    "    return x\n",
    "    \n",
    "def load_one_brain(path):\n",
    "    img=nibabel.load(path)\n",
    "    img=img.get_fdata()\n",
    "    \n",
    "    print(img.shape)\n",
    "    img=crop(img,50,178,50,178,12,140)\n",
    "    print(img.shape)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618c2512-feb7-410b-b8d2-a8ba0671849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_slice(volume,slice_num):\n",
    "    plt.imshow(volume[:,:,slice_num],cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ccfa76-eda6-4aa4-82c5-95f54ffbbeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 240, 155)\n",
      "(128, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "basic_brain=load_one_brain(\"/usr/local/faststorage/BraTS19_Data/Training/Data/BraTS19_2013_3_1/BraTS19_2013_3_1_flair.nii.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14615c8a-2387-4687-8c52-2ef3ff5bc983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_slice(downsample(basic_brain,2),32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91350b36-2a7c-4398-bc9b-4e0ae4ff8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim=64,k=4):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(1, k, kernel_size=3, stride=1)\n",
    "        self.conv2=nn.Conv2d(k, 2*k, kernel_size=3, stride=2,padding=1)\n",
    "        self.conv3=nn.Conv2d(2*k, 4*k, kernel_size=3, stride=2,padding=1)\n",
    "        self.conv4=nn.Conv2d(4*k, 6*k, kernel_size=3, stride=2,padding=1)\n",
    "        #self.fc1=nn.Linear(6*k*8*8,64)\n",
    "        self.z_mean_fc=nn.Linear(16*6*k,z_dim)\n",
    "        self.z_std_fc=nn.Linear(16*6*k,z_dim)\n",
    "        self.ap=nn.AdaptiveAvgPool2d((4,4))\n",
    "        self.lrelu=nn.LeakyReLU()\n",
    "        self.in1=nn.InstanceNorm2d(k)\n",
    "        self.in2=nn.InstanceNorm2d(2*k)\n",
    "        self.in3=nn.InstanceNorm2d(4*k)\n",
    "        self.in4=nn.InstanceNorm2d(6*k)\n",
    "    def forward(self, xs):\n",
    "        x=self.lrelu(self.in1(self.conv1(xs)))\n",
    "        x=self.lrelu(self.in2(self.conv2(x)))\n",
    "        x=self.lrelu(self.in3(self.conv3(x)))\n",
    "        x=self.lrelu(self.in4(self.conv4(x)))\n",
    "        x=self.ap(x)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        z_loc=self.z_mean_fc(x)\n",
    "        z_scale=torch.exp(self.z_std_fc(x))\n",
    "        return z_loc,z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf435e8f-66c1-4eb8-b26d-0eea2076c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim=64,k=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.deconv1=torch.nn.ConvTranspose2d(z_dim, 6*k, kernel_size=3, stride=2,padding=1,output_padding=1)\n",
    "        self.deconv2=torch.nn.ConvTranspose2d(6*k, 4*k, kernel_size=3, stride=2,padding=1,output_padding=1)\n",
    "        self.deconv3=torch.nn.ConvTranspose2d(4*k, 2*k, kernel_size=3, stride=2,padding=1,output_padding=1)\n",
    "        self.deconv4=torch.nn.ConvTranspose2d(2*k, 2*k, kernel_size=3, stride=2,padding=1,output_padding=1)\n",
    "        self.deconv5=torch.nn.ConvTranspose2d(2*k, 2*k, kernel_size=3, stride=2,padding=1,output_padding=1)\n",
    "        self.deconv6=torch.nn.ConvTranspose2d(2*k, 2*k, kernel_size=3, stride=2,padding=1,output_padding=1)\n",
    "        self.conv1=torch.nn.Conv2d(2*k,2,kernel_size=5,stride=1,padding=\"same\")\n",
    "        self.in1=nn.InstanceNorm2d(6*k)\n",
    "        self.in2=nn.InstanceNorm2d(4*k)\n",
    "        self.in3=nn.InstanceNorm2d(2*k)\n",
    "        self.in4=nn.InstanceNorm2d(2*k)\n",
    "        self.in5=nn.InstanceNorm2d(2*k)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.lrelu=nn.LeakyReLU()\n",
    "\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "    def forward(self, z):\n",
    "        z=z.unsqueeze(2).unsqueeze(3)\n",
    "        #print(z)\n",
    "        x = self.lrelu(self.in1(self.deconv1(z)))\n",
    "        x = self.lrelu(self.in2(self.deconv2(x)))\n",
    "        \n",
    "        x = self.lrelu(self.in3(self.deconv3(x)))\n",
    "        x = self.lrelu(self.in4(self.deconv4(x)))\n",
    "        x = self.lrelu((self.deconv5(x)))\n",
    "        x = self.lrelu(self.deconv6(x))\n",
    "        #print(x)\n",
    "        x=self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        recon_loc=x[:,0,...]\n",
    "        recon_var=torch.exp(x[:,1,...])\n",
    "        \n",
    "        \n",
    "        return recon_loc,recon_var\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b512bd02-129a-4096-9dac-b03c5108a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalAffineTransform(ConditionalTransformModule):\n",
    "    def __init__(self, context_nn, event_dim=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.event_dim = event_dim\n",
    "        self.context_nn = context_nn\n",
    "\n",
    "    def condition(self, context):\n",
    "        loc, log_scale = self.context_nn(context)\n",
    "        scale = torch.exp(log_scale)\n",
    "        ac = transforms.AffineTransform(loc, scale, event_dim=self.event_dim)\n",
    "        return ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9096f1ef-777d-49eb-bae2-9db11cf27a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=1,z_dim=64,k=4):\n",
    "        super().__init__()\n",
    "        fc_1=nn.Linear(input_size,4)\n",
    "        fc_z_mean=nn.Linear(4,1)\n",
    "        fc_z_std=nn.Linear(4,1)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "    def forward(self,z,y):\n",
    "        x=self.relu(fc_1(y))\n",
    "        affine_1=fc_z_mean(x)\n",
    "        affine_2=fc_z_std(x)\n",
    "        return affine_1,affine_2\n",
    "class Estimator(nn.Module):\n",
    "    def __init__(self, input_size=64,z_dim=1,k=4):\n",
    "        super().__init__()\n",
    "        self.fc_1=nn.Linear(input_size,16)\n",
    "        self.fc_z_mean=nn.Linear(16,1)\n",
    "        self.fc_z_std=nn.Linear(16,1)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "    def forward(self,z):\n",
    "        x=self.relu(self.fc_1(z))\n",
    "        affine_1=self.fc_z_mean(x)\n",
    "        affine_2=torch.exp(self.fc_z_std(x))\n",
    "        return affine_1,affine_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "26a5ef30-a3f0-4b11-9581-72a13d9ce4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, z_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(z_dim,k=4)\n",
    "        self.encoder.apply(init_weights)\n",
    "        self.decoder = Decoder(z_dim+1,k=4)\n",
    "        self.decoder.apply(init_weights)\n",
    "\n",
    "        #self.prior_net.train()\n",
    "        #self.generation_net.train()\n",
    "        self.z_dim=z_dim\n",
    "        #self.recognition_net = Encoder(z_dim,k=4)\n",
    "        self.age_nn=MLP(input_size=1,z_dim=4)\n",
    "        #self.age_flow_components = ConditionalAffineTransform(context_nn=age_nn, event_dim=0)\n",
    "        #self.age_flow_transforms = [self.age_flow_components]\n",
    "        self.age_estimator=Estimator(input_size=64,z_dim=1)\n",
    "    #@pyro_method\n",
    "    #def pgm_model(self):\n",
    "\n",
    "        #return {'age': age_base_dist}\n",
    "        \n",
    "    def model(self, xs,age=None):\n",
    "            # register this pytorch module and all of its sub-modules with pyro\n",
    "            \n",
    "            pyro.module(\"decoder\",self.decoder)\n",
    "            with pyro.plate(\"data\",xs.shape[0]):\n",
    "                \n",
    "                #obs=pgm_model()\n",
    "                age_mean=xs.new_ones(torch.Size((xs.shape[0], 1)))*60.8\n",
    "                age_base_scale=xs.new_ones(torch.Size((xs.shape[0], 1)))*12.89\n",
    "                age_dist = dist.Normal(age_mean, age_base_scale).to_event(1)\n",
    "                age = pyro.sample('age', age_dist)\n",
    "                z_loc = xs.new_zeros(torch.Size((xs.shape[0], self.z_dim)))\n",
    "                z_scale = z_scale = xs.new_ones(torch.Size((xs.shape[0], self.z_dim)))\n",
    "                z_base_dist = dist.Normal(z_loc, z_scale)\n",
    "                z = pyro.sample('latent',z_base_dist.to_event(1))\n",
    "                ctx=torch.cat([z,age],-1)               \n",
    "                loc,var = self.decoder(ctx)\n",
    "                #print(var)\n",
    "                pyro.sample('obs', dist.Normal(loc,var).to_event(3), obs=xs)\n",
    "                #return loc\n",
    "    def guide(self, xs,age=None):\n",
    "            pyro.module(\"encoder\",self.encoder)\n",
    "            with pyro.plate(\"data\",xs.shape[0]):\n",
    "                z_loc, z_scale = self.encoder(xs)\n",
    "                pyro.sample('latent', dist.Normal(z_loc, z_scale).to_event(1))\n",
    "                age_loc,age_scale=self.age_estimator(z_loc)\n",
    "                pyro.sample('age', dist.Normal(age_loc,age_scale).to_event(1))\n",
    "    def reconstruct(self,xs,age):\n",
    "        z_loc, z_scale = self.encoder(xs)\n",
    "        print(z_loc)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        ctx=torch.cat([z,age],-1)\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc,var=self.decoder(ctx)\n",
    "        return loc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c9c5638c-29db-48cf-97ba-4e8ea1d05d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.cuda.current_device()\n",
    "pyro.clear_param_store()\n",
    "my_VAE=CVAE()\n",
    "my_VAE.to(device)\n",
    "#optimizer = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "optimizer = pyro.optim.Adam({\"lr\": .001})\n",
    "svi = SVI(my_VAE.model, my_VAE.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c270a875-df53-4be1-ad6e-32c586b08573",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= build_dataset(root_dir='/usr/local/faststorage/BraTS19_Data/',dataset_type=\"train\",mode=MODALS[\"T1\"])\n",
    "train_loader=VolumeLoader(dataset=train_dataset,root_dir='/usr/local/faststorage/BraTS19_Data/',dataset_type=\"train\",mode=MODALS[\"T1\"])\n",
    "train_pydl=torch.utils.data.DataLoader(train_loader,batch_size=24,num_workers=16,shuffle=True)\n",
    "\n",
    "val_dataset= build_dataset(root_dir='/usr/local/faststorage/BraTS19_Data/',dataset_type=\"val\",mode=MODALS[\"T1\"])\n",
    "val_loader=VolumeLoader(dataset=val_dataset,root_dir='/usr/local/faststorage/BraTS19_Data/',dataset_type=\"val\",mode=MODALS[\"T1\"])\n",
    "val_pydl=torch.utils.data.DataLoader(val_loader,batch_size=4,num_workers=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c7ee5cd8-873d-4b1f-9061-ef46186d99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(pydl):\n",
    "    loss=0.\n",
    "    for i,data in enumerate(train_pydl):\n",
    "        \n",
    "        img=data[\"MRI\"]\n",
    "        img=img[:,50:178:2,50:178:2,63]\n",
    "        img=img-torch.mean(img,axis=(1,2),keepdims=True)\n",
    "        img=img/torch.std(img,axis=(1,2),keepdims=True)\n",
    "        img=img.unsqueeze(1).type(torch.FloatTensor)\n",
    "        \n",
    "        loss+=svi.step(xs=img.cuda(),age=data[\"Age\"].type(torch.FloatTensor).view(-1,1).cuda())\n",
    "    return loss\n",
    "def precompute(pydl):\n",
    "    loss=0.\n",
    "    ages=[]\n",
    "    for i,data in enumerate(train_pydl):\n",
    "        ages.append(data[\"Age\"].numpy())\n",
    "    print(np.mean(np.asarray(ages)))\n",
    "    print(np.std(np.asarray(ages)))\n",
    "                    \n",
    "def evaluate(pydl):\n",
    "    loss=0.\n",
    "    for i,data in enumerate(train_pydl):\n",
    "        img=data[\"MRI\"]\n",
    "        img=img[:,50:178:2,50:178:2,63]\n",
    "        img=img-torch.mean(img,axis=(1,2),keepdims=True)\n",
    "        img=img/torch.std(img,axis=(1,2),keepdims=True)\n",
    "        img=img.unsqueeze(1).type(torch.FloatTensor)\n",
    "        loss = svi.evaluate_loss(xs=img.cuda(), age=data[\"Age\"].type(torch.FloatTensor).view(-1,1).cuda())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42766a2-7c88-463e-847c-74311f5aeca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Training loss is:35329873.32692146\n",
      "Validation loss is:3375229.6083984375\n",
      "3375229.6083984375\n",
      "epoch 1\n",
      "Training loss is:33433123.458423615\n",
      "Validation loss is:3314892.5723876953\n",
      "3314892.5723876953\n",
      "epoch 2\n",
      "Training loss is:32847410.982257843\n",
      "Validation loss is:3247286.3530273438\n",
      "3247286.3530273438\n",
      "epoch 3\n",
      "Training loss is:32026113.042453766\n",
      "Validation loss is:3140834.6798095703\n",
      "3140834.6798095703\n",
      "epoch 4\n",
      "Training loss is:30579826.44121933\n",
      "Validation loss is:2960835.763305664\n",
      "2960835.763305664\n",
      "epoch 5\n",
      "Training loss is:28803090.810661316\n",
      "Validation loss is:2911938.762939453\n",
      "2911938.762939453\n",
      "epoch 6\n",
      "Training loss is:27391920.24053383\n",
      "Validation loss is:2629136.2662353516\n",
      "2629136.2662353516\n",
      "epoch 7\n",
      "Training loss is:26506552.441249847\n",
      "Validation loss is:2725895.53717041\n",
      "epoch 8\n",
      "Training loss is:25953653.169620514\n",
      "Validation loss is:2502890.217895508\n",
      "2502890.217895508\n",
      "epoch 9\n",
      "Training loss is:25470800.762216568\n",
      "Validation loss is:2611451.7365112305\n",
      "epoch 10\n",
      "Training loss is:25146650.691928864\n",
      "Validation loss is:2520828.210205078\n",
      "epoch 11\n",
      "Training loss is:24868696.661010742\n",
      "Validation loss is:2429959.3018188477\n",
      "2429959.3018188477\n",
      "epoch 12\n",
      "Training loss is:24594181.714242935\n",
      "Validation loss is:2406245.7083740234\n",
      "2406245.7083740234\n",
      "epoch 13\n",
      "Training loss is:24408166.232089996\n",
      "Validation loss is:2496270.7309570312\n",
      "epoch 14\n",
      "Training loss is:24255254.882801056\n",
      "Validation loss is:2406034.512817383\n",
      "2406034.512817383\n",
      "epoch 15\n",
      "Training loss is:24081543.657819748\n",
      "Validation loss is:2343646.422241211\n",
      "2343646.422241211\n",
      "epoch 16\n",
      "Training loss is:24011506.87430954\n",
      "Validation loss is:2561555.5107421875\n",
      "epoch 17\n",
      "Training loss is:23864171.6751194\n",
      "Validation loss is:2285751.6713867188\n",
      "2285751.6713867188\n",
      "epoch 18\n",
      "Training loss is:23724006.31293106\n",
      "Validation loss is:2299961.115234375\n",
      "epoch 19\n",
      "Training loss is:23640918.348615646\n",
      "Validation loss is:2448548.935546875\n",
      "epoch 20\n",
      "Training loss is:23525720.461652756\n",
      "Validation loss is:2330752.555541992\n",
      "epoch 21\n",
      "Training loss is:23460771.751161575\n",
      "Validation loss is:2288908.528564453\n",
      "epoch 22\n",
      "Training loss is:23372241.145219803\n",
      "Validation loss is:2313136.7790527344\n",
      "epoch 23\n",
      "Training loss is:23342767.17782402\n",
      "Validation loss is:2342191.951904297\n",
      "epoch 24\n",
      "Training loss is:23207316.515914917\n",
      "Validation loss is:2270021.9885253906\n",
      "2270021.9885253906\n",
      "epoch 25\n",
      "Training loss is:23130804.605674744\n",
      "Validation loss is:2138972.5239257812\n",
      "2138972.5239257812\n",
      "epoch 26\n",
      "Training loss is:23037960.564706802\n",
      "Validation loss is:2454349.7044067383\n",
      "epoch 27\n",
      "Training loss is:23000345.64104271\n",
      "Validation loss is:2205327.028869629\n",
      "epoch 28\n",
      "Training loss is:22950124.934841156\n",
      "Validation loss is:2349150.7478027344\n",
      "epoch 29\n",
      "Training loss is:22898950.567020416\n",
      "Validation loss is:2287411.4559936523\n",
      "epoch 30\n",
      "Training loss is:22843159.95361519\n",
      "Validation loss is:2219904.4260253906\n",
      "epoch 31\n",
      "Training loss is:22839488.391267776\n",
      "Validation loss is:2281599.104614258\n",
      "epoch 32\n",
      "Training loss is:22847302.65221405\n",
      "Validation loss is:2331565.924194336\n",
      "epoch 33\n",
      "Training loss is:22716528.47890663\n",
      "Validation loss is:2136078.2771606445\n",
      "2136078.2771606445\n",
      "epoch 34\n",
      "Training loss is:22663235.468070984\n",
      "Validation loss is:2237081.769165039\n",
      "epoch 35\n",
      "Training loss is:22572160.769273758\n",
      "Validation loss is:2297183.3358154297\n",
      "epoch 36\n",
      "Training loss is:22524688.535959244\n",
      "Validation loss is:2444405.6147460938\n",
      "epoch 37\n"
     ]
    }
   ],
   "source": [
    "train_loss=[]\n",
    "val_loss=[]\n",
    "val_loss_best=np.inf\n",
    "#precompute(train_pydl)\n",
    "for i in range(1000):\n",
    "    print(\"epoch \" +str(i))\n",
    "    train_loss.append(train(train_pydl))\n",
    "    print(\"Training loss is:\" +str(train_loss[-1]))\n",
    "    val_loss.append(evaluate(val_pydl))\n",
    "    print(\"Validation loss is:\" +str(val_loss[-1]))\n",
    "    if val_loss[-1]<val_loss_best:\n",
    "        val_loss_best=val_loss[-1]\n",
    "        print(val_loss_best)\n",
    "        torch.save(my_VAE, \"./best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc63463-b18b-46d3-b41f-d22e76a03e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cea5ce-f6da-4dda-9443-9ecea60bafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(my_VAE.reconstruct(torch.Tensor(resha_slice),age=torch.Tensor([[50]]).view(1,1)).detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce574474-4db4-4c7e-9a89-f2eff76e93e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d8a52-fac9-46a5-a68b-964c5be483e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfb585-ed0f-4e48-9bf5-57ae7d9a1b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497bf9b0-64d6-496a-a767-f2d35b192aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5babd3-81e4-4e02-a0df-df4755e96229",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee962a-8abb-4544-8694-a661f57a9eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b600e27-9512-4681-b529-7308981df766",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
